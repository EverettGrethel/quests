{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208b744-ea69-414c-bc47-5bf0c94d44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "from ase.io import read\n",
    "from pyace import create_multispecies_basis_config\n",
    "from pyace.activelearning import compute_B_projections\n",
    "from quests.entropy import perfect_entropy\n",
    "\n",
    "H_star = {\n",
    "    \"Graphene\":        4.245179458166078,\n",
    "    \"Diamond\":         4.318381910272738,\n",
    "    \"Graphite\":        5.6085074467370095,\n",
    "    \"Nanotubes\":       7.0282707526691715,\n",
    "    \"Fullerenes\":      8.67911004440742,\n",
    "    \"Defects\":         9.531933892473084,\n",
    "    \"Surfaces\":        9.823139796211981,\n",
    "    \"Liquid\":          11.61485589283075,\n",
    "    \"Amorphous_Bulk\":  12.183809856122803,\n",
    "}\n",
    "\n",
    "# Optional weights (defaults to 1.0)\n",
    "W = {k: 1.0 for k in H_star}\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "def optimize_bandwidth_global(descriptor_dict, H_star, W, h0=None, span=5.0, tol=1e-3, maxit=60):\n",
    "    \"\"\"\n",
    "    descriptor_dict: {name: np.ndarray (N_atoms, N_feat)}\n",
    "    H_star:          {name: float} ground-truth entropies\n",
    "    W:               {name: float} weights\n",
    "    h0:              initial guess for h (>0). If None, use scale heuristic.\n",
    "    span:            multiplicative span around h0 for bracketing; i.e. [h0/span, h0*span]\n",
    "    returns: (h_opt, report_dict)\n",
    "    \"\"\"\n",
    "    # If no h0, build a simple scale from descriptors (median L2 across a tiny subset)\n",
    "    if not h0 or not np.isfinite(h0) or h0 <= 0:\n",
    "        samples = []\n",
    "        for X in descriptor_dict.values():\n",
    "            if X.size == 0: continue\n",
    "            n = min(len(X), 64)\n",
    "            idx = np.random.choice(len(X), size=n, replace=False)\n",
    "            Xi = X[idx]\n",
    "            # pairwise norms on a tiny subset for scale\n",
    "            diffs = Xi[:n//2] - Xi[n//2:n]\n",
    "            if diffs.size:\n",
    "                samples.append(np.median(np.linalg.norm(diffs, axis=1)))\n",
    "        h0 = np.median(samples) if samples else 1.0\n",
    "        if not np.isfinite(h0) or h0 <= 0:\n",
    "            h0 = 1.0\n",
    "\n",
    "    # Work in log-space to keep h>0\n",
    "    u0 = math.log(h0)\n",
    "    # Bracket in log-space using symmetric span\n",
    "    u_lo = math.log(h0 / span)\n",
    "    u_hi = math.log(h0 * span)\n",
    "\n",
    "    def L_of_u(u):\n",
    "        h = math.exp(u)\n",
    "        loss = 0.0\n",
    "        for name, X in descriptor_dict.items():\n",
    "            if name not in H_star: \n",
    "                continue\n",
    "            H = perfect_entropy(X, h=h, batch_size=batch_size)\n",
    "            diff = H - H_star[name]\n",
    "            loss += W.get(name, 1.0) * (diff * diff)\n",
    "        return loss\n",
    "\n",
    "    res = minimize_scalar(\n",
    "        L_of_u,\n",
    "        bracket=(u_lo, u0, u_hi),   # Brent accepts a 3-pt bracket\n",
    "        method=\"brent\",\n",
    "        options={\"xtol\": tol, \"maxiter\": maxit},\n",
    "    )\n",
    "    u_opt = res.x\n",
    "    h_opt = float(math.exp(u_opt))\n",
    "\n",
    "    # Build a small report\n",
    "    report = {\n",
    "        \"success\": bool(res.success),\n",
    "        \"message\": str(res.message),\n",
    "        \"nit\": int(res.nfev),\n",
    "        \"u_opt\": float(u_opt),\n",
    "        \"h0\": float(h0),\n",
    "        \"u0\": float(u0),\n",
    "        \"bracket\": [float(u_lo), float(u0), float(u_hi)],\n",
    "        \"final_loss\": float(res.fun),\n",
    "    }\n",
    "    return h_opt, report\n",
    "\n",
    "# --------- Inside your sweep loop, replace the bandwidth section ---------\n",
    "# 1) Precompute descriptors once per dataset (with the *data* basis)\n",
    "frames_cache = {name: read(f\"/home/grethel/dev/quests/examples/gap20/{name}.xyz\", index=\":\")\n",
    "                for name in H_star}\n",
    "\n",
    "# ... within your for params in sweep: loop ...\n",
    "# Build data basis (C) once per tier\n",
    "data_basis_config = make_ace_config(params=params, elements=[\"C\"])\n",
    "data_basis = create_multispecies_basis_config(data_basis_config)\n",
    "\n",
    "# Compute descriptors once per dataset for this basis\n",
    "descriptor_dict = {}\n",
    "for name, frames in frames_cache.items():\n",
    "    X = compute_B_projections(data_basis, frames)[0]\n",
    "    descriptor_dict[name] = X\n",
    "\n",
    "# Get initial guess from your FCC heuristic (as you asked)\n",
    "fcc_basis_config = make_ace_config(params=params, elements=[\"Au\"])  # or [\"Au\",\"Ag\"] if you prefer\n",
    "fcc_basis = create_multispecies_basis_config(fcc_basis_config)\n",
    "h0 = fcc_strain_heuristic(fcc_basis, supercell=3)  # your current function\n",
    "\n",
    "# 2) Brent minimize the global loss\n",
    "h_opt, opt_report = optimize_bandwidth_global(\n",
    "    descriptor_dict=descriptor_dict,\n",
    "    H_star=H_star,\n",
    "    W=W,\n",
    "    h0=h0,\n",
    "    span=5.0,     # try 3–10 if needed\n",
    "    tol=1e-3,\n",
    "    maxit=60\n",
    ")\n",
    "\n",
    "# 3) Evaluate entropies at the optimized bandwidth and serialize\n",
    "entry = {\n",
    "    \"basis_config\": data_basis_config,   # the basis actually used for datasets\n",
    "    \"elements_data\": [\"C\"],\n",
    "    \"bandwidth_init\": float(h0),\n",
    "    \"bandwidth_opt\": float(h_opt),\n",
    "    \"optimizer\": opt_report,\n",
    "    \"entropy\": {},\n",
    "    \"entropy_error\": {},                 # H - H*\n",
    "}\n",
    "\n",
    "for name, X in descriptor_dict.items():\n",
    "    H = perfect_entropy(X, h=h_opt, batch_size=batch_size)\n",
    "    entry[\"entropy\"][name] = float(H)\n",
    "    if name in H_star:\n",
    "        entry[\"entropy_error\"][name] = float(H - H_star[name])\n",
    "\n",
    "# append to jsonl\n",
    "with open(results_path, \"a\") as f:\n",
    "    f.write(json.dumps(entry) + \"\\n\")\n",
    "    f.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ba409-54d5-4776-85c9-7a5f01a0a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from ase.io import read\n",
    "from ase.build import bulk, make_supercell\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from pyace import create_multispecies_basis_config\n",
    "from pyace.activelearning import compute_B_projections\n",
    "\n",
    "from quests.entropy import perfect_entropy  # and diversity if you need it\n",
    "\n",
    "# -------------------------- User Config ---------------------------------\n",
    "\n",
    "# GAP-20 dataset names you want to include in the global fit\n",
    "DATASETS = [\n",
    "    \"Graphene\",\n",
    "    \"Diamond\",\n",
    "    \"Graphite\",\n",
    "    \"Nanotubes\",\n",
    "    \"Fullerenes\",\n",
    "    \"Defects\",\n",
    "    \"Surfaces\",\n",
    "    \"Liquid\",\n",
    "    \"Amorphous_Bulk\",\n",
    "]\n",
    "# DATASETS = [\n",
    "#     \"Graphene\",\n",
    "#     \"Graphite\",\n",
    "#     \"Nanotubes\"\n",
    "# ]\n",
    "\n",
    "# Provide ground-truth entropies for each dataset (REPLACE these with real values)\n",
    "H_STAR = {\n",
    "    \"Graphene\":        4.245179458166078,\n",
    "    \"Diamond\":         4.318381910272738,\n",
    "    \"Graphite\":        5.6085074467370095,\n",
    "    \"Nanotubes\":       7.0282707526691715,\n",
    "    \"Fullerenes\":      8.67911004440742,\n",
    "    \"Defects\":         9.531933892473084,\n",
    "    \"Surfaces\":        9.823139796211981,\n",
    "    \"Liquid\":          11.61485589283075,\n",
    "    \"Amorphous_Bulk\":  12.183809856122803,\n",
    "}\n",
    "\n",
    "# Optional per-dataset weights (defaults to 1.0 if key missing)\n",
    "W = {k: 1.0 for k in H_STAR}\n",
    "\n",
    "# Where the GAP-20 XYZ files live\n",
    "GAP20_DIR = \"/home/grethel/dev/quests/examples/gap20\"\n",
    "\n",
    "# Output JSONL\n",
    "RESULTS_PATH = \"/home/grethel/dev/quests/sweep_results/sweep_global_brent.jsonl\"\n",
    "os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "\n",
    "# Batch size for entropy calculation\n",
    "BATCH_SIZE = 10_000\n",
    "\n",
    "# FCC heuristic (initial guess) probe settings\n",
    "HEURISTIC_ELEMENTS = (\"Au\",)   # or (\"Au\",\"Ag\") if you want multi-component probe\n",
    "HEURISTIC_SUPERCELL = 3        # supercell multiplier for the FCC probe\n",
    "\n",
    "# Fidelity sweep (tiers of nrad/lmax)\n",
    "# SWEEP = [\n",
    "#     {\"nrad\": [4],           \"lmax\": [4]},             # F0\n",
    "#     {\"nrad\": [6, 3],        \"lmax\": [6, 3]},          # F1\n",
    "#     {\"nrad\": [8, 4, 2],     \"lmax\": [8, 6, 2]},       # F2\n",
    "#     {\"nrad\": [10, 6, 3],    \"lmax\": [10, 8, 4]},      # F3\n",
    "#     {\"nrad\": [12, 8, 4],    \"lmax\": [12, 10, 6]},     # F4\n",
    "# ]\n",
    "SWEEP = [\n",
    "    {\"nrad\": [12, 8, 4],    \"lmax\": [12, 10, 6]},     # F4\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def make_ace_config(params: dict, elements: list, rcut: float = 5.5, dcut: float = 0.2, ndensity: int = 1):\n",
    "    \"\"\"Build a python-ACE basis config for the given elements and (nrad,lmax) tiers.\"\"\"\n",
    "    return {\n",
    "        \"elements\": elements,\n",
    "        \"embeddings\": {\n",
    "            \"ALL\": {\n",
    "                \"npot\": \"FinnisSinclairShiftedScaled\",\n",
    "                \"fs_parameters\": [3.0, 0.8],\n",
    "                \"ndensity\": int(ndensity),  # must be int\n",
    "            }\n",
    "        },\n",
    "        \"bonds\": {\n",
    "            \"ALL\": {\n",
    "                \"radbase\": \"SBessel\",\n",
    "                \"radparameters\": [rcut],\n",
    "                \"rcut\": rcut,\n",
    "                \"dcut\": dcut,\n",
    "            }\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"ALL\": {\n",
    "                \"nradmax_by_orders\": params[\"nrad\"],\n",
    "                \"lmax_by_orders\":    params[\"lmax\"],\n",
    "            }\n",
    "        },\n",
    "        \"deltaSplineBins\": 5e-5,\n",
    "    }\n",
    "\n",
    "\n",
    "def fcc_strain_heuristic(basis, species=\"Au\", a=3.58, supercell=1, strain=0.99):\n",
    "    \"\"\"Single-species FCC heuristic; returns L2 between one atom before/after strain.\"\"\"\n",
    "    fcc1 = bulk(species, \"fcc\", a=a, cubic=True)\n",
    "    if supercell and supercell > 1:\n",
    "        T = (np.eye(3, dtype=int) * int(supercell))\n",
    "        fcc1 = make_supercell(fcc1, T)\n",
    "\n",
    "    fcc2 = fcc1.copy()\n",
    "    fcc2.set_cell(strain * fcc2.cell, scale_atoms=True)\n",
    "\n",
    "    X1 = compute_B_projections(basis, [fcc1])[0]  # (N_atoms, N_feat)\n",
    "    X2 = compute_B_projections(basis, [fcc2])[0]\n",
    "    if X1.shape[0] == 0 or X2.shape[0] == 0:\n",
    "        raise ValueError(\"Empty descriptors in FCC heuristic; check basis elements and rcut/dcut.\")\n",
    "\n",
    "    # Compare atom 0 (deterministic in cubic cell)\n",
    "    return float(np.linalg.norm(X1[0] - X2[0]))\n",
    "\n",
    "\n",
    "def _default_scale_from_descriptors(descriptor_dict):\n",
    "    \"\"\"Fallback to build a positive initial h if heuristic fails or not provided.\"\"\"\n",
    "    samples = []\n",
    "    for X in descriptor_dict.values():\n",
    "        if X is None or X.size == 0:\n",
    "            continue\n",
    "        n = min(len(X), 64)\n",
    "        if n < 2:\n",
    "            continue\n",
    "        idx = np.random.choice(len(X), size=n, replace=False)\n",
    "        Xi = X[idx]\n",
    "        n2 = n // 2\n",
    "        diffs = Xi[:n2] - Xi[n2:2*n2]\n",
    "        if diffs.size > 0:\n",
    "            samples.append(np.median(np.linalg.norm(diffs, axis=1)))\n",
    "    h0 = np.median(samples) if samples else 1.0\n",
    "    if not np.isfinite(h0) or h0 <= 0:\n",
    "        h0 = 1.0\n",
    "    return float(h0)\n",
    "\n",
    "\n",
    "def optimize_bandwidth_global(descriptor_dict, H_star, W, h0=None,\n",
    "                              span=5.0, tol=1e-3, maxit=200):\n",
    "    \"\"\"\n",
    "    Global bandwidth fit: minimize L(h)=sum_d w_d [H_d(h)-H*_d]^2.\n",
    "    Optimize u=log h with 'bounded' (golden-section). No bracketing min needed.\n",
    "    \"\"\"\n",
    "    # fallback scale if no good h0\n",
    "    def _default_scale():\n",
    "        samples = []\n",
    "        for X in descriptor_dict.values():\n",
    "            if X is None or X.size == 0: \n",
    "                continue\n",
    "            n = min(len(X), 64)\n",
    "            if n < 2: \n",
    "                continue\n",
    "            idx = np.random.choice(len(X), size=n, replace=False)\n",
    "            Xi = X[idx]\n",
    "            n2 = n // 2\n",
    "            diffs = Xi[:n2] - Xi[n2:2*n2]\n",
    "            if diffs.size:\n",
    "                samples.append(np.median(np.linalg.norm(diffs, axis=1)))\n",
    "        val = np.median(samples) if samples else 1.0\n",
    "        return float(val if np.isfinite(val) and val > 0 else 1.0)\n",
    "\n",
    "    if h0 is None or not np.isfinite(h0) or h0 <= 0:\n",
    "        h0 = _default_scale()\n",
    "\n",
    "    # bounds in log-space\n",
    "    w = math.log(span)\n",
    "    u0 = math.log(h0)\n",
    "    u_lo, u_hi = u0 - w, u0 + w\n",
    "\n",
    "    def L_of_u(u):\n",
    "        h = math.exp(u)\n",
    "        loss = 0.0\n",
    "        for name, X in descriptor_dict.items():\n",
    "            if name not in H_star:\n",
    "                continue\n",
    "            H = perfect_entropy(X, h=h, batch_size=BATCH_SIZE)\n",
    "            d = H - H_star[name]\n",
    "            loss += W.get(name, 1.0) * (d * d)\n",
    "        return loss\n",
    "\n",
    "    res = minimize_scalar(\n",
    "        L_of_u,\n",
    "        bounds=(u_lo, u_hi),\n",
    "        method=\"bounded\",\n",
    "        options={\"xatol\": tol, \"maxiter\": maxit},\n",
    "    )\n",
    "    u_opt = float(res.x)\n",
    "    h_opt = float(math.exp(u_opt))\n",
    "    report = {\n",
    "        \"success\": bool(res.success),\n",
    "        \"message\": str(res.message),\n",
    "        \"nfev\": int(res.nfev),\n",
    "        \"u_opt\": u_opt,\n",
    "        \"h0\": float(h0),\n",
    "        \"bounds\": [float(u_lo), float(u_hi)],\n",
    "        \"final_loss\": float(res.fun),\n",
    "        \"method\": \"bounded\",\n",
    "    }\n",
    "    return h_opt, report\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Preload frames for all datasets (so we can reuse across tiers)\n",
    "    frames_cache = {}\n",
    "    for name in DATASETS:\n",
    "        path = os.path.join(GAP20_DIR, f\"{name}.xyz\")\n",
    "        frames_cache[name] = read(path, index=\":\")\n",
    "\n",
    "    # Open output for appending (one JSON per tier)\n",
    "    with open(RESULTS_PATH, \"w\") as fout:\n",
    "        for params in SWEEP:\n",
    "            print(f\"\\n>>> Tier params: {params}\")\n",
    "\n",
    "            # Build data basis (carbon) ONCE per tier and cache descriptors per dataset\n",
    "            data_basis_config = make_ace_config(params=params, elements=[\"C\"])\n",
    "            data_basis = create_multispecies_basis_config(data_basis_config)\n",
    "\n",
    "            descriptor_dict = {}\n",
    "            for name, frames in frames_cache.items():\n",
    "                X = compute_B_projections(data_basis, frames)[0]\n",
    "                descriptor_dict[name] = X\n",
    "                print(f\"  {name}: descriptors shape = {X.shape}\")\n",
    "\n",
    "            # Initial guess from FCC heuristic using HEURISTIC_ELEMENTS\n",
    "            heuristic_basis_config = make_ace_config(params=params, elements=list(HEURISTIC_ELEMENTS))\n",
    "            heuristic_basis = create_multispecies_basis_config(heuristic_basis_config)\n",
    "            try:\n",
    "                h0 = fcc_strain_heuristic(\n",
    "                    heuristic_basis,\n",
    "                    species=HEURISTIC_ELEMENTS[0],\n",
    "                    a=3.58,\n",
    "                    supercell=HEURISTIC_SUPERCELL,\n",
    "                    strain=0.99,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  Heuristic failed ({e}); falling back to descriptor scale.\")\n",
    "                h0 = None\n",
    "\n",
    "            print(f\"  Initial bandwidth guess h0 = {h0 if h0 is not None else '(auto)'}\")\n",
    "\n",
    "            # Optimize global bandwidth\n",
    "            h_opt, opt_report = optimize_bandwidth_global(\n",
    "                descriptor_dict=descriptor_dict,\n",
    "                H_star=H_STAR,\n",
    "                W=W,\n",
    "                h0=h0,\n",
    "                span=5.0,\n",
    "                tol=1e-3,\n",
    "                maxit=60,\n",
    "            )\n",
    "            if (not rpt[\"success\"]) or (abs(rpt[\"u_opt\"] - rpt[\"bounds\"][0]) < 1e-6) or (abs(rpt[\"u_opt\"] - rpt[\"bounds\"][1]) < 1e-6):\n",
    "                print(\"Span failed, trying different span...\")\n",
    "                h_opt, opt_report = optimize_bandwidth_global(\n",
    "                    descriptor_dict=descriptor_dict,\n",
    "                    H_star=H_STAR,\n",
    "                    W=W,\n",
    "                    h0=h0,\n",
    "                    span=100.0,\n",
    "                    tol=1e-3,\n",
    "                    maxit=60,\n",
    "                )\n",
    "                \n",
    "            print(f\"  Optimized bandwidth h* = {h_opt:.6f} | loss = {opt_report['final_loss']:.6e}\")\n",
    "\n",
    "            # Evaluate entropies at the optimized bandwidth and compute errors\n",
    "            entropy_at_opt = {}\n",
    "            entropy_err = {}\n",
    "            for name, X in descriptor_dict.items():\n",
    "                H = perfect_entropy(X, h=h_opt, batch_size=BATCH_SIZE)\n",
    "                entropy_at_opt[name] = float(H)\n",
    "                if name in H_STAR:\n",
    "                    entropy_err[name] = float(H - H_STAR[name])\n",
    "\n",
    "            # Write JSONL entry (per tier)\n",
    "            entry = {\n",
    "                \"basis_config\": data_basis_config,    # the basis used for the datasets\n",
    "                \"elements_data\": [\"C\"],\n",
    "                \"tier_params\": params,\n",
    "                \"bandwidth_init\": float(opt_report[\"h0\"]),\n",
    "                \"bandwidth_opt\": float(h_opt),\n",
    "                \"optimizer\": opt_report,\n",
    "                \"entropy\": entropy_at_opt,\n",
    "                \"entropy_error\": entropy_err,\n",
    "            }\n",
    "            fout.write(json.dumps(entry) + \"\\n\")\n",
    "            fout.flush()\n",
    "\n",
    "    print(f\"\\n✅ Done. Results written to: {RESULTS_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
